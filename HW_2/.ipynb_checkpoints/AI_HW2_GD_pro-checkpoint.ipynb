{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Murcha1990/ML_AI24/blob/main/Hometasks/Pro/AI_HW2_GD_pro.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Домашнее задание 2: Градиентный спуск (pro)**\n",
        "\n",
        "В этом задании две части - теоретическая и практическая. Теорию можно набирать, используя LaTex или просто решать на листочке, сфотографировать и отправить вместе с заполненным ноутбуком в anytask.\n",
        "\n",
        "Максимальный балл за домашнее задание - 10."
      ],
      "metadata": {
        "id": "lJ1Hwqs18fSZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Часть 1 (теоретическая)\n",
        "\n",
        "## Задание 1 (**1.5 балла**)\n",
        "\n",
        "В случае одномерной Ridge-регрессии минимизируется функция со штрафом:\n",
        "$$Q(w) = (y-xw)^T(y-xw)+\\lambda w^2,$$\n",
        "где $\\lambda$ - положительный параметр, штрафующий функцию за слишком большие значения $w$.\n",
        "\n",
        "1)  (**0.5 балла**) Найдите производную $\\nabla_w Q(w)$, выведите формулу для оптимального $w$.\n",
        "\n",
        "2) (**0.5 балла**) Найдите вторую производную $\\nabla^2_w Q(w)$. Убедитесь, что мы оказались в точке минимума.\n",
        "\n",
        "3) (**0.5 балла**) Выпишите шаг градиентного спуска в матричном виде."
      ],
      "metadata": {
        "id": "EMZ5My7kiPY_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "`Ваш ответ здесь`\n",
        "\n"
      ],
      "metadata": {
        "id": "LQMI7b9Y9KvW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Часть 2 (практическая).\n",
        "\n",
        "## Задание 2 (**1 балл**)\n",
        "\n",
        "Дана функция: $$f(x) = x\\cdot sin(5x) + 0.1 \\cdot x^2$$\n",
        "\n",
        "Для этой функции:\n",
        "\n",
        "- [ ] Реализуйте (или возьмите рассмотренный на занятии) метод градиетного спуска с условием остановки `stop=1e^-6` и шагом `eta=0.001`. **Градиетный спуск обязательно должен сохранять траекторию движения.**\n",
        "- [ ] Задайте стартовые точки x0, равные 0, 0.5, 1\n",
        "- [ ] Реализуйте нахождение точек минимума и максимума функции $f(x)$ (для нахождения максимума нам нужно в направлении градиента, а не антиградиента)\n",
        "- [ ] Проанализируйте результаты. Предположите, с чем они связаны. Ответьте на вопросы:\n",
        "    - Чему равны значения экстремумов?\n",
        "    - При старте из какой начальной точки найденные экстремумы совпадут?\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "_pGazqdK9Tvf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Функция и её производная\n",
        "def f(x):\n",
        "\n",
        "  \"\"\"Ваш код здесь\"\"\"\n",
        "\n",
        "  return\n",
        "\n",
        "def df(x):\n",
        "\n",
        "    \"\"\"Ваш код здесь\"\"\"\n",
        "\n",
        "    return\n",
        "\n",
        "# Градиентный спуск\n",
        "def gradient_descent(f, df, x0, eta, max_iter=1000, tol=1e-6):\n",
        "\n",
        "    \"\"\"Ваш код здесь\"\"\"\n",
        "\n",
        "    return x, path\n",
        "\n",
        "# Поиск экстремумов\n",
        "def find_points(f, df, x0, eta, max_iter=1000, tol=1e-6):\n",
        "\n",
        "    \"\"\"Ваш код здесь\"\"\"\n",
        "\n",
        "    return"
      ],
      "metadata": {
        "id": "HJbAEs3_-zet"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Задание 3 (**0.5 балла**).\n",
        "Напишите функцию, вычисляющую значение весов в линейной регрессии по точной (аналитически найденной) формуле."
      ],
      "metadata": {
        "id": "lm6_Ln0GoliG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def ols_solution(X, y):\n",
        "    # your code here"
      ],
      "metadata": {
        "id": "ZrVvpU9miOga"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Задание 5 (**1 балл**).\n",
        "Модифицируйте метод градиентного спуска с семинара так, чтобы это теперь был метод стохастического градиентного спуска."
      ],
      "metadata": {
        "id": "_shCsTQ1pVcU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def stochastic_gradient_descent(X, y, learning_rate, iterations):\n",
        "    # your code here"
      ],
      "metadata": {
        "id": "fTZWxz1zpb9R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Задание 6 (**3 балла**).\n",
        "* **(0 баллов)**. Скопируйте метод градиентного спуска из семинара в этот ноутбук.\n",
        "\n",
        "* **(0.5 балла)**. Обучите линейную регрессию на данных, сгенерированных ниже, тремя методами (по точной формуле, с помощью GD и с помощью SGD) на данных для задачи регрессии (см. код). Для GD и SGD используйте learning_rate = 0.01, iterations=10000.\n",
        "\n",
        "* **(0.5 балла)**. С помощью каждого метода сделайте предсказание (на всех данных), вычислите качество предсказания r2 (from sklearn.metrics import r2_score). Для получения предсказания можете использовать функцию predict с семинара.\n",
        "\n",
        "\n",
        "Ответьте на следующие вопросы (каждый вопрос - **0.5 балла**):\n",
        "\n",
        "1) все ли методы справились с нахождением минимума? если нет, то почему какой-то из методов не справился?\n",
        "\n",
        "2) сравните время работы методов (используйте библиотеку time): замеряйте время работы соответствующей написанной вами функции.\n",
        "\n",
        "3) для методов GD и SGD нарисуйте графики (для каждого свой) зависимости ошибки (loss) от номера итерации.\n",
        "\n",
        "4) какой метод успешнее всего справился с задачей? (т.е. r2 наибольший)."
      ],
      "metadata": {
        "id": "WnRlUa9Npi9o"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import make_regression\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "X, y, _ = make_regression(n_samples=100000,#number of samples\n",
        "                          n_features=10,#number of features\n",
        "                          n_informative=8,#number of useful features\n",
        "                          noise=100,#bias and standard deviation of the guassian noise\n",
        "                          coef=True,#true coefficient used to generated the data\n",
        "                          random_state=123)\n",
        "\n",
        "X = pd.DataFrame(data=X, columns=np.arange(0, X.shape[1]))\n",
        "X[10] = X[6] + X[7] + np.random.random()*0.01"
      ],
      "metadata": {
        "id": "LBu41KSpqbbI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# your code here"
      ],
      "metadata": {
        "id": "f1SE0-oUtVlO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "your text here"
      ],
      "metadata": {
        "id": "2zrdVgQrtcAn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Задание 7 (1.5 балла).\n",
        "\n",
        "* **(0.5 балла)**. Добавьте в функцию из задания 5 L2-регуляризацию и, соответственно, новый аргумент - коэффициент при регуляризаторе.\n",
        "\n",
        "* **(1 балл)**. На сгенерированных выше данных обучите модифицированный алгоритм SGD с регуляризацией: в цикле перебирайте значения коэффициента регуляризации от 0.1 до 1 с шагом 0.1. Для каждого значения обучите модель и сделайте предсказание, выведите значение r2. Для какого значения коэффициента регуляризации получилось наилучшее качество r2, почему?"
      ],
      "metadata": {
        "id": "wYmzzJVV54zT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# your code here"
      ],
      "metadata": {
        "id": "b_hC1Ehi612d"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Задание 8 (2 балла)\n",
        "\n",
        "Реализуйте на выбор одну из оптимизаций градиентного спуска - Momentum или  AdaGrad. Необходимые выкладки вы можете подсмотреть [здесь](https://education.yandex.ru/handbook/ml/article/optimizaciya-v-ml)."
      ],
      "metadata": {
        "id": "zMUiRyCiCB3E"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def modified_grad_descent(X, y, learning_rate, iterations): # добавьте в аргументы также специфические для метода оптимизации гиперпараметры\n",
        "    # your code here"
      ],
      "metadata": {
        "id": "ID9ervHwCyc7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Бонус\n",
        "\n",
        "## Задание 9 (0.5 балла)\n",
        "\n",
        "Существует [метод Ньютона](https://en.wikipedia.org/wiki/Newton%27s_method_in_optimization) (метод второго порядка) для поиска минимума функции.\n",
        "\n",
        "Известно, что методы второго порядка точнее, чем методы первого порядка (то есть те, которые используют только первую производную для оптимизации). Как вы думаете, почему в оптимизации функций потерь все формулы используют только производные первого порядка? Почему не используют метод Ньютона?"
      ],
      "metadata": {
        "id": "tP182Wbyh8xg"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "`Ваш ответ здесь`"
      ],
      "metadata": {
        "id": "QBHOY5Ngi_C2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Часть Котобонус** 🐈\n",
        "\n",
        "Ура-ура, с вами снова кот, который может добавить вам баллы!\n",
        "- 0.25, если вы угадаете, чей он\n",
        "- 0.05 если просто предположите\n",
        "\n",
        "**Подсказка:** кот одной из наших замечательных ассистенток!\n",
        "\n",
        "\n",
        "[Кайфующий кот](https://ibb.co/qxjz2tp)"
      ],
      "metadata": {
        "id": "u7hKRuViF7hv"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Hchpz56wGEzA"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}